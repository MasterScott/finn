{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to CustomOp\n",
    "\n",
    "\n",
    "Need to create subclasses of `CustomOp` to provide execution, code generation and other functionality in FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " 'execute_node',\n",
       " 'get_nodeattr',\n",
       " 'get_nodeattr_allowed_values',\n",
       " 'get_nodeattr_def',\n",
       " 'get_nodeattr_types',\n",
       " 'infer_node_datatype',\n",
       " 'make_shape_compatible_op',\n",
       " 'set_nodeattr',\n",
       " 'verify_node']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.custom_op.base import CustomOp\n",
    "dir(CustomOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(note: the `CustomOp` base class has moved into `finn-base`: https://github.com/Xilinx/finn-base/blob/dev/src/finn/custom_op/base.py -- the `finn` Docker container already has `finn-base` set up as a dependency)\n",
    "\n",
    "Some points of importance:\n",
    "\n",
    "1. `CustomOp` instances (in Python) are not meant to store any data, only provide functionality on top of data stored in ONNX. Each `CustomOp` instance has a member `self.onnx_node` which gives access to the ONNX `NodeProto` with attributes. There is also a custom attribute setter/getter system in `CustomOp` to make this process easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `CustomOp` subclasses need to implement the methods above (those not starting with underscore)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To be discoverable in the custom op register, `CustomOp` subclasses must set the `domain` field to the name of the Python module they appear in. For instance, to use the custom `Im2Col` op type from [here](https://github.com/Xilinx/finn-base/blob/dev/src/finn/custom_op/general/im2col.py), the ONNX node must use `domain=finn.custom_op.general`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple CustomOp Example\n",
    "\n",
    "Let's make a simple CustomOp that raises its input to a given exponent (specified as attribute). For now it'll only work in Python, but later we'll add C++ execution capability too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import helper\n",
    "import numpy as np\n",
    "\n",
    "class MyPythonPowerOp(CustomOp):\n",
    "    \n",
    "    # here we use the CustomOp attribute system to make it easier\n",
    "    # to set/get custom attributes on this node\n",
    "    def get_nodeattr_types(self):\n",
    "        return {\n",
    "            # each entry is:\n",
    "            # name of attribute : (dtype, required, default value)\n",
    "            # dtype follows the ONNX attribute protobuf so\n",
    "            # \"i\" is int, \"s\" is string, \"f\" is float,\n",
    "            # \"ints\" is a list of integers...\n",
    "            # also good practice to document what each attribute does here:\n",
    "            \n",
    "            # which integer power to raise the input to\n",
    "            \"exponent\" : (\"i\", True, 0),\n",
    "            # execution mode : currently only python\n",
    "            \"exec_mode\" : (\"s\", True, \"python\"),\n",
    "        }\n",
    "    \n",
    "    # return an ONNX node that has the same shape inference behavior\n",
    "    # here we want in shape = out shape, so we use the ONNX ReLU\n",
    "    # node to mimic its shape inference behavior\n",
    "    # we have access to the entire ModelWrapper to help make this decision\n",
    "    # (the parameter called model)\n",
    "    def make_shape_compatible_op(self, model):\n",
    "        node = self.onnx_node\n",
    "        # make a Relu node connected to the same in-out tensors to get\n",
    "        # shape inference\n",
    "        # a general-purpose alternative is to use a Constant node that \n",
    "        # produces the desired shape\n",
    "        return helper.make_node(\"Relu\", [node.input[0]], [node.output[0]])\n",
    "\n",
    "    # used for FINN DataType inference: set the output tensors' datatypes\n",
    "    # accordingly for this node\n",
    "    # here we assume input datatype = output datatype\n",
    "    # we have access to the entire ModelWrapper to help make this decision\n",
    "    # (the parameter called model)\n",
    "    def infer_node_datatype(self, model):\n",
    "        node = self.onnx_node\n",
    "        # data type stays the same\n",
    "        dtype = model.get_tensor_datatype(node.input[0])\n",
    "        model.set_tensor_datatype(node.output[0], dtype)\n",
    "    \n",
    "    # execute this node\n",
    "    # context: used for both input and output, dictionary of named\n",
    "    #          tensors\n",
    "    # graph: the ONNX GraphProto (ModelWrapper.graph), generally \n",
    "    #         not needed to execute a single node\n",
    "    def execute_node(self, context, graph):\n",
    "        exec_mode = self.get_nodeattr(\"exec_mode\")\n",
    "        if exec_mode == \"python\":\n",
    "            # get names of node input and output tensors\n",
    "            i_name = self.onnx_node.input[0]\n",
    "            o_name = self.onnx_node.output[0]\n",
    "            # grab input tensor from context\n",
    "            i_tensor = context[i_name]\n",
    "            # get which power to raise to from attribute\n",
    "            expnt = self.get_nodeattr(\"exponent\")\n",
    "            # compute and put output into context\n",
    "            o_tensor = np.power(i_tensor, expnt)\n",
    "            context[o_name] = o_tensor\n",
    "        else:\n",
    "            raise Exception(\"Only python exec_mode is supported\")\n",
    "        \n",
    "    # can use to do a sanity check of all the node's properties\n",
    "    # optional, not implemented here\n",
    "    def verify_node(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure our custom op is available, it needs to be registered. The best practice for this is to create a submodule under `finn.custom_op` which includes a `custom_op` dictionary that maps strings (op names) to classes (op implementations). Since we're in a Jupyter notebook we'll just hijack it at runtme like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.custom_op.general as general\n",
    "general.custom_op[\"MyPythonPowerOp\"] = MyPythonPowerOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which custom ops are registered under this submodule by looking at the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DebugMarker': finn.custom_op.general.debugmarker.DebugMarker,\n",
       " 'QuantAvgPool2d': finn.custom_op.general.quantavgpool2d.QuantAvgPool2d,\n",
       " 'MaxPoolNHWC': finn.custom_op.general.maxpoolnhwc.MaxPoolNHWC,\n",
       " 'StreamingDataflowPartition': finn.custom_op.general.streamingdataflowpartition.StreamingDataflowPartition,\n",
       " 'MultiThreshold': finn.custom_op.general.multithreshold.MultiThreshold,\n",
       " 'XnorPopcountMatMul': finn.custom_op.general.xnorpopcount.XnorPopcountMatMul,\n",
       " 'Im2Col': finn.custom_op.general.im2col.Im2Col,\n",
       " 'MyPythonPowerOp': __main__.MyPythonPowerOp}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general.custom_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Try Out our CustomOp\n",
    "\n",
    "We'll manually build a small ONNX graph containing our node in order to try out some of the functionality. This would normally go into the unit test for this CustomOp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from onnx import TensorProto\n",
    "\n",
    "def make_graph(ishape, exp, op_type = \"MyPythonPowerOp\"):\n",
    "    inp = helper.make_tensor_value_info(\n",
    "        \"inp\", TensorProto.FLOAT, ishape\n",
    "    )\n",
    "    outp = helper.make_tensor_value_info(\n",
    "        \"outp\", TensorProto.FLOAT, ishape\n",
    "    )\n",
    "\n",
    "    custom_node = helper.make_node(\n",
    "        # op type string in ONNX, what we used to register the custom op\n",
    "        op_type,\n",
    "        # name of input tensor\n",
    "        [\"inp\"],\n",
    "        # name of output tensor\n",
    "        [\"outp\"],\n",
    "        # specify domain s.t. FINN can find our op under this submodule\n",
    "        domain=\"finn.custom_op.general\",\n",
    "        # set up attributes\n",
    "        exponent = int(exp),\n",
    "        exec_mode = \"python\"\n",
    "    )\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=[custom_node], name=\"custom_graph\", inputs=[inp], outputs=[outp]\n",
    "    )\n",
    "    model = helper.make_model(graph, producer_name=\"custom-model\")\n",
    "    return ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[input: \"inp\"\n",
       "output: \"outp\"\n",
       "op_type: \"MyPythonPowerOp\"\n",
       "attribute {\n",
       "  name: \"exec_mode\"\n",
       "  s: \"python\"\n",
       "  type: STRING\n",
       "}\n",
       "attribute {\n",
       "  name: \"exponent\"\n",
       "  i: 2\n",
       "  type: INT\n",
       "}\n",
       "domain: \"finn.custom_op.general\"\n",
       "]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a small graph with our custom op\n",
    "input_shape = (1, 2, 4)\n",
    "ret_model = make_graph(input_shape, 2)\n",
    "ret_model.model.graph.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1., -7.,  5.,  3.],\n",
       "        [-5.,  4.,  6.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.core.datatype import DataType\n",
    "from finn.util.basic import gen_finn_dt_tensor\n",
    "\n",
    "# generate a random input of e.g signed 4-bit values\n",
    "random_input = gen_finn_dt_tensor(DataType.INT4, input_shape)\n",
    "random_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outp': array([[[ 1., 49., 25.,  9.],\n",
       "         [25., 16., 36.,  0.]]], dtype=float32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.core.onnx_exec import execute_onnx\n",
    "\n",
    "# run with FINN's execute_onnx\n",
    "inp_dict = {\"inp\" : random_input}\n",
    "ret = execute_onnx(ret_model, inp_dict)\n",
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CustomOp with C++ Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir, CppBuilder\n",
    "import subprocess\n",
    "\n",
    "# derive from our previous example\n",
    "class MyMixedPowerOp(MyPythonPowerOp):\n",
    "    \n",
    "    # here we use the CustomOp attribute system to make it easier\n",
    "    # to set/get custom attributes on this node\n",
    "    def get_nodeattr_types(self):\n",
    "        return {\n",
    "            # each entry is:\n",
    "            # name of attribute : (dtype, required, default value)\n",
    "            # dtype follows the ONNX attribute protobuf so\n",
    "            # \"i\" is int, \"s\" is string, \"f\" is float,\n",
    "            # \"ints\" is a list of integers...\n",
    "            # also good practice to document what each attribute does here:\n",
    "            \n",
    "            # which integer power to raise the input to\n",
    "            \"exponent\" : (\"i\", True, 0),\n",
    "            # execution mode : python or c++\n",
    "            \"exec_mode\" : (\"s\", True, \"python\"),\n",
    "            # code generation directory\n",
    "            \"codegen_dir\" : (\"s\", False, \"\"),\n",
    "        }\n",
    "    \n",
    "    def my_custom_cpp_gen(self):\n",
    "        codegen_dir = make_build_dir(prefix=\"my_custom_op\")\n",
    "        # set attribute for codegen dir\n",
    "        self.set_nodeattr(\"codegen_dir\", codegen_dir)\n",
    "        # generate some C++ code\n",
    "        cpp_code = \"\"\"\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "using namespace std;\n",
    "#define EXPONENT %d\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    ifstream infile(\"input.txt\");\n",
    "    ofstream outfile(\"output.txt\");\n",
    "    \n",
    "    float elem;\n",
    "    while (infile >> elem)\n",
    "    {\n",
    "        float res = 1.0;\n",
    "        for(int i=0; i < EXPONENT; i++) {\n",
    "            res *= elem;\n",
    "        }\n",
    "        outfile << res << \"\\\\n\";\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "        \"\"\" % (self.get_nodeattr(\"exponent\"))\n",
    "        with open(codegen_dir+\"/top.cpp\", \"w\") as f:\n",
    "            f.write(cpp_code)\n",
    "        builder = CppBuilder()\n",
    "        # to enable additional debug features please uncommand the next line\n",
    "        builder.append_includes(\"--std=c++11\")\n",
    "        builder.append_includes(\"-O3\")\n",
    "        builder.append_sources(codegen_dir + \"/*.cpp\")\n",
    "        builder.set_executable_path(codegen_dir + \"/node_model\")\n",
    "        builder.build(codegen_dir)\n",
    "    \n",
    "    # execute this node\n",
    "    # context: used for both input and output, dictionary of named\n",
    "    #          tensors\n",
    "    # graph: the ONNX GraphProto (ModelWrapper.graph), generally \n",
    "    #         not needed to execute a single node\n",
    "    def execute_node(self, context, graph):\n",
    "        exec_mode = self.get_nodeattr(\"exec_mode\")\n",
    "        # get names of node input and output tensors\n",
    "        i_name = self.onnx_node.input[0]\n",
    "        o_name = self.onnx_node.output[0]\n",
    "        # grab input tensor from context\n",
    "        i_tensor = context[i_name]\n",
    "        # get which power to raise to from attribute\n",
    "        expnt = self.get_nodeattr(\"exponent\")\n",
    "        if exec_mode == \"python\":\n",
    "            # compute and put output into context\n",
    "            o_tensor = np.power(i_tensor, expnt)\n",
    "            context[o_name] = o_tensor\n",
    "        elif exec_mode == \"c++\":\n",
    "            build_dir = self.get_nodeattr(\"codegen_dir\")\n",
    "            # save input as txt, could preprocess, change layout etc..\n",
    "            np.savetxt(build_dir+\"/input.txt\", i_tensor.flatten())\n",
    "            bash_command = [\"./node_model\"]\n",
    "            proc_run = subprocess.Popen(bash_command, cwd=build_dir, stdout=subprocess.PIPE)\n",
    "            proc_run.communicate()\n",
    "            o_tensor = np.loadtxt(build_dir+\"/output.txt\")\n",
    "            o_tensor = o_tensor.reshape(i_tensor.shape)\n",
    "            context[o_name] = o_tensor\n",
    "        else:\n",
    "            raise Exception(\"Only python and c++ exec_mode is supported\")\n",
    "        \n",
    "    # can use to do a sanity check of all the node's properties\n",
    "    # optional, not implemented here\n",
    "    def verify_node(self):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[input: \"inp\"\n",
       "output: \"outp\"\n",
       "op_type: \"MyMixedPowerOp\"\n",
       "attribute {\n",
       "  name: \"exec_mode\"\n",
       "  s: \"python\"\n",
       "  type: STRING\n",
       "}\n",
       "attribute {\n",
       "  name: \"exponent\"\n",
       "  i: 2\n",
       "  type: INT\n",
       "}\n",
       "domain: \"finn.custom_op.general\"\n",
       "]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register our new op\n",
    "general.custom_op[\"MyMixedPowerOp\"] = MyMixedPowerOp\n",
    "\n",
    "# make graph with new op\n",
    "mixedop_graph = make_graph(input_shape, 2, op_type = \"MyMixedPowerOp\")\n",
    "mixedop_graph.graph.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available functions: ['__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_cache', '_abc_negative_cache', '_abc_negative_cache_version', '_abc_registry', 'execute_node', 'get_nodeattr', 'get_nodeattr_allowed_values', 'get_nodeattr_def', 'get_nodeattr_types', 'infer_node_datatype', 'make_shape_compatible_op', 'my_custom_cpp_gen', 'onnx_node', 'set_nodeattr', 'verify_node']\n",
      "codegen_dir: \n",
      "exec_mode: python\n"
     ]
    }
   ],
   "source": [
    "from finn.custom_op.registry import getCustomOp\n",
    "\n",
    "# get FINN wrapper for this node, with all the functionality\n",
    "op_inst = getCustomOp(mixedop_graph.model.graph.node[0])\n",
    "print(\"Available functions: \" + str(dir(op_inst)))\n",
    "# query some attributes\n",
    "print(\"codegen_dir: \" + op_inst.get_nodeattr(\"codegen_dir\"))\n",
    "print(\"exec_mode: \" + op_inst.get_nodeattr(\"exec_mode\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a code generation transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from finn.transformation.base import Transformation\n",
    "# can derive from NodeLocalTransformation for faster (parallel) execution\n",
    "from finn.transformation.base import NodeLocalTransformation\n",
    "import os\n",
    "\n",
    "class MyNodeLocalCodeGen(NodeLocalTransformation):\n",
    "    \n",
    "    # will get called (possibly in parallel) for each node\n",
    "    def applyNodeLocal(self, node):\n",
    "        # keep track whether we changed anything\n",
    "        modified_graph = False\n",
    "        # check node type before we do anything\n",
    "        if node.op_type == \"MyMixedPowerOp\":\n",
    "            # get FINN wrapper for this node, with all the functions\n",
    "            op_inst = getCustomOp(node)\n",
    "            if not os.path.isdir(op_inst.get_nodeattr(\"codegen_dir\")):\n",
    "                # call the codegen function we defined\n",
    "                # this will modify the underlying node by setting attribute\n",
    "                op_inst.my_custom_cpp_gen()\n",
    "                # codegen function modifies attribute\n",
    "                modified_graph = True\n",
    "        # important: must return modified_graph = False at some point\n",
    "        # otherwise transformation will run in infinite loop!\n",
    "        return (node, modified_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedop_graph_new = mixedop_graph.transform(MyNodeLocalCodeGen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/finn_dev_maltanar/my_custom_opva5okbax\n"
     ]
    }
   ],
   "source": [
    "new_op_inst = getCustomOp(mixedop_graph_new.graph.node[0])\n",
    "codegen_dir = new_op_inst.get_nodeattr(\"codegen_dir\")\n",
    "print(codegen_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile.sh  node_model\ttop.cpp\r\n"
     ]
    }
   ],
   "source": [
    "! ls {codegen_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#include <iostream>\r\n",
      "#include <fstream>\r\n",
      "using namespace std;\r\n",
      "#define EXPONENT 2\r\n",
      "\r\n",
      "int main(int argc, char **argv) {\r\n",
      "    ifstream infile(\"input.txt\");\r\n",
      "    ofstream outfile(\"output.txt\");\r\n",
      "    \r\n",
      "    float elem;\r\n",
      "    while (infile >> elem)\r\n",
      "    {\r\n",
      "        float res = 1.0;\r\n",
      "        for(int i=0; i < EXPONENT; i++) {\r\n",
      "            res *= elem;\r\n",
      "        }\r\n",
      "        outfile << res << \"\\n\";\r\n",
      "    }\r\n",
      "\r\n",
      "    return 0;\r\n",
      "}\r\n",
      "        "
     ]
    }
   ],
   "source": [
    "! cat {codegen_dir}/top.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually generate input and run C++ node model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"7.0 8.0 9.0\" > {codegen_dir}/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {codegen_dir}; ./node_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\r\n",
      "64\r\n",
      "81\r\n"
     ]
    }
   ],
   "source": [
    "! cat {codegen_dir}/output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm {codegen_dir}/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use FINN execution flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6., -3., -3.,  3.],\n",
       "        [-7., -3.,  3.,  6.]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a random input of e.g signed 4-bit values\n",
    "random_input = gen_finn_dt_tensor(DataType.INT4, input_shape)\n",
    "random_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outp': array([[[36.,  9.,  9.,  9.],\n",
       "         [49.,  9.,  9., 36.]]], dtype=float32)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run with FINN's execute_onnx, custom node will use Python execution\n",
    "new_op_inst.set_nodeattr(\"exec_mode\", \"python\")\n",
    "inp_dict = {\"inp\" : random_input}\n",
    "ret = execute_onnx(mixedop_graph_new, inp_dict)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outp': array([[[36.,  9.,  9.,  9.],\n",
       "         [49.,  9.,  9., 36.]]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run with FINN's execute_onnx, custom node will use Python execution\n",
    "new_op_inst.set_nodeattr(\"exec_mode\", \"c++\")\n",
    "ret = execute_onnx(mixedop_graph_new, inp_dict)\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
